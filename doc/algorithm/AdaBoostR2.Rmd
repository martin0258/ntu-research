AdaBoost.R2 (Drucker, 1997)
========================================================

### Input
- **training data**: D = $(x_1, y_1),...,(x_n, y_n)$ where $x_i \in X, y_i \in R$
- **maximum number of iterations**: N
- **a base learning algorithm**: learner
- **loss function**: linear, square, or exponential

### Precondition
- The base learning algorithm supports training with weights on data.

### Process
First, initialize equal weight on each training sample, i.e., $w_i^1 = 1/n$ for i = 1,...,n.

For t = 1,...,N:
  1. Call learner with the training data D and the weight vector $w^t$, and get a hypothesis $h_t$.
  2. Calculate a loss $L_i$ for each training sample:  
    $D_t = max | h_t(x_i) - y_i |$ for i = 1,...,n  
    Based on choice of loss function:  
      $L_i = | h_t(x_i) - y_i | / D_t$ (linear)  
      $L_i = | h_t(x_i) - y_i |^2 / D_t^2$ (square)  
      $L_i = 1 - exp[ - | h_t(x_i) - y_i | / D_t]$ (exponential)
  3. Calculate an average loss:  
    $\bar{L} = \sum_{i=1}^n L_i*w_i^t$
  4. If $\bar{L} \geq 0.5$, stop and set N = t - 1.
  4. Caculate a measure of predictor confidence:  
    $\beta_t = \bar{L} / (1-\bar{L})$
  5. Update the weight vector:  
    $w_i^{t+1} = w_i^t * \beta_t^{1-L_i}$

### Output
The final hypothesis $h(x)$ is the weighted median of $h_t(x)$ for t=1,...,N  
,using $ln(1/\beta_t)$ as the weight for hypothesis $h_t$.